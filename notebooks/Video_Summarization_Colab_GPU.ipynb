{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "798dac28",
      "metadata": {
        "id": "798dac28"
      },
      "source": [
        "# ðŸŽ¬ Video Summarization with GPU Demo\n",
        "\n",
        "This notebook demonstrates the complete Video Summarization pipeline on **Google Colab with GPU acceleration**.\n",
        "\n",
        "**What you'll do:**\n",
        "1. Download a YouTube video\n",
        "2. Transcribe audio using Whisper (small.en on GPU)\n",
        "3. Summarize the transcript using Qwen 2.5 1.5B GGUF (on GPU)\n",
        "4. Download results as a ZIP file\n",
        "\n",
        "**Estimated time:** 5-10 minutes (first run includes model downloads)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29161ed9",
      "metadata": {
        "id": "29161ed9"
      },
      "source": [
        "## Step 1: Check GPU & Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "ecbd7ca8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecbd7ca8",
        "outputId": "19f42b6f-69c8-4d0e-983a-604ab1557a1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jan 28 12:31:59 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0             32W /   70W |    4036MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Executable: /usr/bin/python3\n",
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         113G   43G   70G  38% /\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi || echo \"No GPU found â€“ using CPU\"\n",
        "\n",
        "import sys\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Executable:\", sys.executable)\n",
        "\n",
        "!df -h /content | head -2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "169b0de7",
      "metadata": {
        "id": "169b0de7"
      },
      "source": [
        "## Step 2: Install System Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "b215dafb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b215dafb",
        "outputId": "7b00aa01-9b57-454b-99da-e60a39d99f94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n"
          ]
        }
      ],
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install -y -qq ffmpeg\n",
        "!ffmpeg -version | head -1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1437989e",
      "metadata": {
        "id": "1437989e"
      },
      "source": [
        "## Step 3: Install Python Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "c93a9a78",
      "metadata": {
        "id": "c93a9a78"
      },
      "outputs": [],
      "source": [
        "!pip install -q faster-whisper\n",
        "!pip install -q llama-cpp-python\n",
        "!pip install -q yt-dlp\n",
        "!pip install -q huggingface-hub\n",
        "!pip install -q torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32f4d8b8",
      "metadata": {
        "id": "32f4d8b8"
      },
      "source": [
        "## Step 4: Setup Hugging Face Token (Optional but Recommended)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "b174fa1b",
      "metadata": {
        "id": "b174fa1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91664e70-a0dc-4f7a-dc6f-f006a68562f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optional: Hugging Face token (helps avoid rate limits)\n",
            "HF token (press Enter to skip): Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Proceeding without token\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "print(\"Optional: Hugging Face token (helps avoid rate limits)\")\n",
        "hf_token = getpass(\"HF token (press Enter to skip): \")\n",
        "\n",
        "if hf_token.strip():\n",
        "    os.environ[\"HF_TOKEN\"] = hf_token\n",
        "    print(\"Token set\")\n",
        "else:\n",
        "    print(\"Proceeding without token\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f035567",
      "metadata": {
        "id": "5f035567"
      },
      "source": [
        "## Step 5: Optional - Mount Google Drive (for storage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "271f54c1",
      "metadata": {
        "id": "271f54c1"
      },
      "outputs": [],
      "source": [
        "# Optional: Mount Google Drive for model storage\n",
        "# Uncomment the next 3 lines if you want to save models to Drive\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# print(\"âœ“ Google Drive mounted at /content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "614e7ddd",
      "metadata": {
        "id": "614e7ddd"
      },
      "source": [
        "## Step 6: GPU Detection & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "44510287",
      "metadata": {
        "id": "44510287",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01511d56-a2d2-4203-def3-34f481505a7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: True\n",
            "Device: Tesla T4\n",
            "Compute type: float16\n"
          ]
        }
      ],
      "source": [
        "import torch, time, json\n",
        "from pathlib import Path\n",
        "\n",
        "GPU_AVAILABLE = torch.cuda.is_available()\n",
        "DEVICE = \"cuda\" if GPU_AVAILABLE else \"cpu\"\n",
        "DEVICE_NAME = torch.cuda.get_device_name(0) if GPU_AVAILABLE else \"CPU\"\n",
        "COMPUTE_TYPE = \"float16\" if GPU_AVAILABLE else \"int8\"\n",
        "\n",
        "print(\"GPU:\", GPU_AVAILABLE)\n",
        "print(\"Device:\", DEVICE_NAME)\n",
        "print(\"Compute type:\", COMPUTE_TYPE)\n",
        "\n",
        "timings = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e220e9c",
      "metadata": {
        "id": "1e220e9c"
      },
      "source": [
        "## Step 7: Download Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "4277eb6d",
      "metadata": {
        "id": "4277eb6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92a3d95c-9c8a-44b6-9be0-434b7d71f272"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Qwen GGUFâ€¦\n",
            "Qwen model path: /content/models/models--Qwen--Qwen2.5-1.5B-Instruct-GGUF/snapshots/91cad51170dc346986eccefdc2dd33a9da36ead9/qwen2.5-1.5b-instruct-q4_k_m.gguf\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model_dir = Path(\"/content/models\")\n",
        "model_dir.mkdir(exist_ok=True)\n",
        "\n",
        "qwen_repo = \"Qwen/Qwen2.5-1.5B-Instruct-GGUF\"\n",
        "qwen_file = \"qwen2.5-1.5b-instruct-q4_k_m.gguf\"\n",
        "\n",
        "print(\"Downloading Qwen GGUFâ€¦\")\n",
        "\n",
        "qwen_model_path = hf_hub_download(\n",
        "    repo_id=qwen_repo,\n",
        "    filename=qwen_file,\n",
        "    cache_dir=str(model_dir),\n",
        "    token=os.environ.get(\"HF_TOKEN\")\n",
        ")\n",
        "\n",
        "print(\"Qwen model path:\", qwen_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "801794dd",
      "metadata": {
        "id": "801794dd"
      },
      "source": [
        "## Step 8: Load Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "26755e87",
      "metadata": {
        "id": "26755e87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69560607-33b6-429e-e57b-20842e156766"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Whisper (small.en)â€¦\n",
            "Whisper loaded\n"
          ]
        }
      ],
      "source": [
        "from faster_whisper import WhisperModel\n",
        "\n",
        "print(\"Loading Whisper (small.en)â€¦\")\n",
        "t0 = time.time()\n",
        "\n",
        "whisper_model = WhisperModel(\n",
        "    \"small.en\",\n",
        "    device=DEVICE,\n",
        "    compute_type=COMPUTE_TYPE\n",
        ")\n",
        "\n",
        "timings[\"load_whisper\"] = time.time() - t0\n",
        "print(\"Whisper loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "import multiprocessing\n",
        "\n",
        "print(\"Loading Qwenâ€¦\")\n",
        "t0 = time.time()\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=qwen_model_path,\n",
        "    n_ctx=8192,\n",
        "    n_gpu_layers=35 if GPU_AVAILABLE else 0,\n",
        "    n_threads=multiprocessing.cpu_count(),\n",
        "    n_batch=512,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "timings[\"load_qwen\"] = time.time() - t0\n",
        "print(\"Qwen loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF61Lo4zix8f",
        "outputId": "cf13e479-08d4-45a0-f438-fc0c526b74bf"
      },
      "id": "qF61Lo4zix8f",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Qwenâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qwen loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aab9872",
      "metadata": {
        "id": "9aab9872"
      },
      "source": [
        "## Step 9: Download YouTube Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "59443537",
      "metadata": {
        "id": "59443537",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02218ef8-cf24-489a-f9a0-e7bc0b5be666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio ready: /content/audio/audio.wav\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "YOUTUBE_URL = \"https://youtu.be/e-P5IFTqB98\"\n",
        "\n",
        "audio_dir = Path(\"/content/audio\")\n",
        "audio_dir.mkdir(exist_ok=True)\n",
        "audio_path = audio_dir / \"audio.wav\"\n",
        "\n",
        "if not audio_path.exists():\n",
        "    subprocess.run(\n",
        "        [\n",
        "            \"yt-dlp\",\n",
        "            \"-f\", \"bestaudio/best\",\n",
        "            \"-x\",\n",
        "            \"--audio-format\", \"wav\",\n",
        "            \"--postprocessor-args\", \"-ar 16000 -ac 1\",\n",
        "            \"-o\", str(audio_path),\n",
        "            YOUTUBE_URL\n",
        "        ],\n",
        "        check=True\n",
        "    )\n",
        "\n",
        "print(\"Audio ready:\", audio_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_path # /content/audio/Black Holes Explained â€“Â From Birth to Death.opus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hDcmXAvfpdU",
        "outputId": "01e4ded9-f08d-4aac-ce89-f28d8ab8b5cd"
      },
      "id": "9hDcmXAvfpdU",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/audio/audio.wav')"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8793b88e",
      "metadata": {
        "id": "8793b88e"
      },
      "source": [
        "## Step 10: Transcribe Audio with Whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "d73fc9b8",
      "metadata": {
        "id": "d73fc9b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b47ce7c-0cf5-418c-edfd-9c66820bf080"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribingâ€¦\n",
            "Transcription time: 13.630083560943604\n",
            "Preview:  Black holes are one of the strangest things in existence.  They don't seem to make any sense at all.  Where do they come from?  And what happens if you fall into one?  Stars are incredibly massive collections of mostly hydrogen atoms  that collapse from enormous gas clouds under their own gravity. \n"
          ]
        }
      ],
      "source": [
        "print(\"Transcribingâ€¦\")\n",
        "t0 = time.time()\n",
        "\n",
        "segments, info = whisper_model.transcribe(\n",
        "    str(audio_path),\n",
        "    language=\"en\",\n",
        "    beam_size=1,\n",
        "    vad_filter=True,\n",
        "    condition_on_previous_text=True\n",
        ")\n",
        "\n",
        "transcript = \" \".join(s.text for s in segments)\n",
        "timings[\"transcription\"] = time.time() - t0\n",
        "\n",
        "output_dir = Path(\"/content/output\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "transcript_file = output_dir / \"output_transcript.txt\"\n",
        "transcript_file.write_text(transcript)\n",
        "\n",
        "print(\"Transcription time:\", timings[\"transcription\"])\n",
        "print(\"Preview:\", transcript[:300])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 10.5"
      ],
      "metadata": {
        "id": "Z__nX9KkkgHb"
      },
      "id": "Z__nX9KkkgHb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10.5 â€” Extractive Compression (CPU, fast)\n",
        "\n",
        "!pip install -q scikit-learn\n",
        "\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def compress_chunk_tfidf(text, top_k=8):\n",
        "    \"\"\"\n",
        "    Keep only the most informative sentences using TF-IDF.\n",
        "    Preserves factual density while removing filler.\n",
        "    \"\"\"\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    if len(sentences) <= top_k:\n",
        "        return text\n",
        "\n",
        "    tfidf = TfidfVectorizer(stop_words=\"english\")\n",
        "    X = tfidf.fit_transform(sentences)\n",
        "    scores = X.sum(axis=1).A1\n",
        "\n",
        "    top_idx = scores.argsort()[::-1][:top_k]\n",
        "    top_idx = sorted(top_idx)\n",
        "\n",
        "    return \" \".join(sentences[i] for i in top_idx)"
      ],
      "metadata": {
        "id": "xVc4JnIykfzZ"
      },
      "id": "xVc4JnIykfzZ",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9ba5b930",
      "metadata": {
        "id": "9ba5b930"
      },
      "source": [
        "## Step 11: Chunk Transcript for Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "ed71cbbf",
      "metadata": {
        "id": "ed71cbbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06a303fd-4944-48f5-e2e1-6136eb4b5380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunked transcript into 1 sections\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def chunk_text(text, max_words=3000, overlap=300):\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    chunks, current, count = [], [], 0\n",
        "\n",
        "    for s in sentences:\n",
        "        w = len(s.split())\n",
        "        if count + w > max_words and current:\n",
        "            chunks.append(\" \".join(current))\n",
        "            current = current[-3:]\n",
        "            count = sum(len(x.split()) for x in current)\n",
        "        current.append(s)\n",
        "        count += w\n",
        "\n",
        "    if current:\n",
        "        chunks.append(\" \".join(current))\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(\n",
        "    transcript,\n",
        "    max_words=4500,   # was 3000\n",
        "    overlap=300\n",
        ")\n",
        "\n",
        "print(f\"Chunked transcript into {len(chunks)} sections\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b640a8a5",
      "metadata": {
        "id": "b640a8a5"
      },
      "source": [
        "## Step 12: Summarize with Qwen (Hierarchical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "cf10b454",
      "metadata": {
        "id": "cf10b454",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8883875-4c61-468b-c2a5-16e09145e673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarizing 1 chunks on CUDA...\n",
            "\n",
            "   Chunk 1/1...\r\n",
            "Chunks summarized in 36.9s\n"
          ]
        }
      ],
      "source": [
        "def summarize_chunk(llm, text, max_tokens=110):\n",
        "    compressed = compress_chunk_tfidf(text, top_k=8)\n",
        "\n",
        "    prompt = f\"\"\"<|im_start|>system\n",
        "Extract 5 concise bullet points.\n",
        "Preserve all numbers, units, and named entities exactly.\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "{compressed}\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "    output = llm(\n",
        "        prompt,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0.0,\n",
        "        stop=[\"\\n\\n\", \"<|im_end|>\"],\n",
        "        echo=False\n",
        "    )\n",
        "\n",
        "    return output[\"choices\"][0][\"text\"].strip()\n",
        "\n",
        "\n",
        "print(f\"Summarizing {len(chunks)} chunks on {DEVICE.upper()}...\\n\")\n",
        "start = time.time()\n",
        "\n",
        "summaries = []\n",
        "for i, chunk in enumerate(chunks, 1):\n",
        "    print(f\"   Chunk {i}/{len(chunks)}...\", end=\"\\r\", flush=True)\n",
        "    summaries.append(summarize_chunk(llm, chunk))\n",
        "\n",
        "timings[\"chunk_summaries\"] = time.time() - start\n",
        "print(f\"\\nChunks summarized in {timings['chunk_summaries']:.1f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 12.5"
      ],
      "metadata": {
        "id": "0rVMadZblIA1"
      },
      "id": "0rVMadZblIA1"
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_two(llm, a, b, max_tokens=160):\n",
        "    prompt = f\"\"\"<|im_start|>system\n",
        "Merge and de-duplicate into a clean, factual summary (max 6 bullets).\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "A:\n",
        "{a}\n",
        "\n",
        "B:\n",
        "{b}\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "    out = llm(\n",
        "        prompt,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0.0,\n",
        "        stop=[\"\\n\\n\", \"<|im_end|>\"],\n",
        "        echo=False\n",
        "    )\n",
        "    return out[\"choices\"][0][\"text\"].strip()\n",
        "\n",
        "print(\"Number of summaries before final reduce:\", len(summaries))\n",
        "\n",
        "def tree_reduce(llm, summaries):\n",
        "    # Case 1: single summary â†’ polish only\n",
        "    if len(summaries) == 1:\n",
        "        out = llm(\n",
        "            f\"\"\"<|im_start|>system\n",
        "Polish and slightly refine the summary.\n",
        "Do not add new facts.\n",
        "Preserve numbers and entities exactly.\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "{summaries[0]}\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\",\n",
        "            max_tokens=160,\n",
        "            temperature=0.0,\n",
        "            stop=[\"\\n\\n\", \"<|im_end|>\"],\n",
        "            echo=False\n",
        "        )\n",
        "        return out[\"choices\"][0][\"text\"].strip()\n",
        "\n",
        "    # Case 2: multiple summaries â†’ tree reduce\n",
        "    level = summaries\n",
        "    while len(level) > 1:\n",
        "        next_level = []\n",
        "        for i in range(0, len(level), 2):\n",
        "            if i + 1 < len(level):\n",
        "                next_level.append(merge_two(llm, level[i], level[i + 1]))\n",
        "            else:\n",
        "                next_level.append(level[i])\n",
        "        level = next_level\n",
        "\n",
        "    return level[0]\n",
        "\n",
        "\n",
        "print(\"\\nGenerating final consolidated summary...\")\n",
        "start = time.time()\n",
        "\n",
        "final_summary = tree_reduce(llm, summaries)\n",
        "\n",
        "timings[\"final_summary\"] = time.time() - start\n",
        "# Save final summary\n",
        "summary_file = output_dir / \"final_summary.txt\"\n",
        "summary_file.write_text(final_summary)\n",
        "\n",
        "print(f\"Final summary generated in {timings['final_summary']:.1f}s\")\n",
        "print(\"\\nFinal Summary:\\n\")\n",
        "print(final_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbrzRllilHg5",
        "outputId": "c377c3b4-9fca-4c82-cd42-4d13dc1b3ae8"
      },
      "id": "WbrzRllilHg5",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of summaries before final reduce: 1\n",
            "\n",
            "Generating final consolidated summary...\n",
            "Final summary generated in 0.0s\n",
            "\n",
            "Final Summary:\n",
            "\n",
            "- Stars are massive collections of hydrogen atoms that collapse under gravity, fusing heavier elements until reaching iron.\n",
            "- A singularity is infinitely dense, with all mass concentrated into a single point with no surface or volume.\n",
            "- Black holes have a mass so concentrated that even tiny distances mean millions of times more force.\n",
            "- Smaller black holes can kill you before you enter their event horizon, while supermassive ones allow travel inside.\n",
            "- The largest known supermassive black hole is S5 0014 with 81, 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 13"
      ],
      "metadata": {
        "id": "-1KFnYG2kUbk"
      },
      "id": "-1KFnYG2kUbk"
    },
    {
      "cell_type": "code",
      "source": [
        "audio_duration = info.duration\n",
        "total_time = sum(timings.values())\n",
        "\n",
        "print(\"\\n=== PERFORMANCE ===\")\n",
        "for k, v in timings.items():\n",
        "    print(f\"{k:20s}: {v:.2f}s\")\n",
        "\n",
        "print(\"Audio duration:\", audio_duration, \"s\")\n",
        "print(\"Transcription speed:\",\n",
        "      f\"{audio_duration / timings['transcription']:.1f}x realtime\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL04ZzBOjq4D",
        "outputId": "2e84ae57-4a4f-4bf2-d08e-2cfcc7daecfb"
      },
      "id": "fL04ZzBOjq4D",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== PERFORMANCE ===\n",
            "load_whisper        : 0.49s\n",
            "load_qwen           : 1.43s\n",
            "transcription       : 13.63s\n",
            "chunk_summaries     : 36.93s\n",
            "final_summary       : 0.00s\n",
            "Audio duration: 355.8458125 s\n",
            "Transcription speed: 26.1x realtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 14"
      ],
      "metadata": {
        "id": "3zPEpoXPkW1D"
      },
      "id": "3zPEpoXPkW1D"
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "from datetime import datetime\n",
        "\n",
        "metadata = {\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"device\": DEVICE_NAME,\n",
        "    \"models\": {\n",
        "        \"whisper\": \"small.en\",\n",
        "        \"summarizer\": \"Qwen 2.5 1.5B GGUF\"\n",
        "    },\n",
        "    \"timings\": timings\n",
        "}\n",
        "\n",
        "meta_file = output_dir / \"metadata.json\"\n",
        "meta_file.write_text(json.dumps(metadata, indent=2))\n",
        "\n",
        "zip_path = \"/content/video_summarization_results.zip\"\n",
        "with zipfile.ZipFile(zip_path, \"w\") as z:\n",
        "    z.write(transcript_file, transcript_file.name)\n",
        "    z.write(summary_file, summary_file.name)\n",
        "    z.write(meta_file, meta_file.name)\n",
        "\n",
        "print(\"Saved:\", zip_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYSzhyrSjrRh",
        "outputId": "57d802e6-d127-4143-bb11-068253b9297c"
      },
      "id": "vYSzhyrSjrRh",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/video_summarization_results.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 15"
      ],
      "metadata": {
        "id": "fRJMSZAjkZzQ"
      },
      "id": "fRJMSZAjkZzQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(zip_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "lt8_a3JIjs9t",
        "outputId": "ed41a253-16eb-4de8-f902-6c77f7bd6a50"
      },
      "id": "lt8_a3JIjs9t",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_68a63b94-b55d-446a-86ca-a461094b88b1\", \"video_summarization_results.zip\", 7292)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 16"
      ],
      "metadata": {
        "id": "0_RknmcikbMK"
      },
      "id": "0_RknmcikbMK"
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "del whisper_model, llm\n",
        "gc.collect()\n",
        "\n",
        "if GPU_AVAILABLE:\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Cleanup complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkjLFFtPjubn",
        "outputId": "38386e29-835f-4774-a055-6d664fce6bfa"
      },
      "id": "xkjLFFtPjubn",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleanup complete\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}